# Распределенное обучение c PyTorch

В данном руководстве мы покажем, как запустить распределенное обучение в PyTorch на платформе. Вы можете найти подробную информацию в наших [рецептах](https://github.com/neuromation/ml-recipe-distributed-pytorch) в нашем блоге: [Распределенное обучение с PyTorch - на примере конкурса ответов на вопросы о TensorFlow 2.0](https://blog.neu.ro/blog/distributed-training-in-pytorch-using-an-example-from-the-tensorflow-2-0-question-answering-competition/).

## Типы параллелизма

### Параллелизм модели

Для этого типа параллелизма мы делим модель на логические части. Этими частями могут быть слои сети или компоненты модели, такие как кодировщик и декодировщик. Метод декомпозиции не является необходимым для понимания этого подхода параллелизма.

Каждый компонент расположен на отдельном устройстве и вычисления выполняются последовательно.

При таком подходе трудно добиться значительного ускорения времени обучения, поскольку все вычисления выполняются последовательно. Необходимо также учитывать накладные расходы из-за значительной передачи данных между устройствами.

Параллелизм модели имеет смысл использовать в случае обучения огромных моделей, которые не помещаются на одном устройстве, даже при небольшом размере блоков.

![&#x41F;&#x430;&#x440;&#x430;&#x43B;&#x43B;&#x435;&#x43B;&#x438;&#x437;&#x43C; &#x43C;&#x43E;&#x434;&#x435;&#x43B;&#x438;](../.gitbook/assets/mp.png)

### Параллелизм данных

В отличии от параллелизма модели при данном подходе небольшие фрагменты данных распределяются между несколькими устройствами, т.е. пакет разбивается на маленькие фрагменты и обрабатывается независимо несколькими копиями одной и той же модели.

Организация обучения заключается в следующем. Одно из устройств предназначено для использования в качестве главного устройства, основной задачей которого является сбор градиентов для градиентного спуска с других устройств и обновление весовых коэффициентов для всех копий модели.

![&#x41F;&#x430;&#x440;&#x430;&#x43B;&#x43B;&#x435;&#x43B;&#x438;&#x437;&#x43C; &#x434;&#x430;&#x43D;&#x43D;&#x44B;&#x445;](../.gitbook/assets/dp.png)

Параллелизм данных является универсальным методом и предоставляется большинством популярных библиотек DL для обучения \(DistributedDataParallel в PyTorch\), освобождая пользователей от необходимости синхронизировать данные между устройствами.

## Попробуйте сами

В нашем [репозитории](https://github.com/neuromation/ml-recipe-distributed-pytorch), мы использовали подход параллелизма данных. Репозиторий содержит решение для [конкурса ответов на вопросы о TensorFlow 2.0](https://www.kaggle.com/c/tensorflow2-question-answering), проводимого Kaggle.

Чтобы загрузить набор данных [Google’s Natural Questions](https://ai.google.com/research/NaturalQuestions/dataset) и запустить наше решение, Вы должны принять пользовательское соглашение Kaggle. Если вы хотите повторно использовать наш рецепт в качестве шаблона для проведения распределенного обучения на платформе, мы также предоставляем `DummyDataset`, который не требует загрузки каких-либо данных.

Для запуска рецепта из `DummyDataset`, необходимо выполнить следующие шаги:

1. [Зарегистрируйтесь](https://app.ml.cloud.mts.ru) и [установите клиент CLI](../first-steps/getting-started.md)
2. Клонируйте [репозиторий](https://github.com/neuromation/ml-recipe-distributed-pytorch)
3. Выполните  
   `neuro-flow build myimage`

   `neuro-flow mkvolumes`

   `neuro-flow upload ALL`

4. Выполните `scripts/run_distributed_on_platform.sh`

Данный подход является общим, поэтому Вы можете использовать наше решение в качестве шаблона для распределенного обучения Ваших моделей в PyTorch.

