# Распределенное обучение c PyTorch

В данном руководстве мы покажем, как запустить распределенное обучение в PyTorch на Neu.ro. Вы можете найти подробную информацию в наших [рецептах](https://github.com/neuromation/ml-recipe-distributed-pytorch) в нашем блоге: [Распределенное обучение с PyTorch - на примере конкурса ответов на вопросы о TensorFlow 2.0](https://blog.neu.ro/blog/distributed-training-in-pytorch-using-an-example-from-the-tensorflow-2-0-question-answering-competition/).

## Типы параллелизма

### Параллелизм модели

Для этого типа параллелизма мы делим модель на логические части. Этими частями могут быть слои сети или компоненты модели, такие как кодировщик и декодеровщик. Метод декомпозиции не является необходимым для понимания этого подхода параллелизма.

Каждый компонент расположен на отдельном устройстве и вычисления выполняются последовательно.

При таком подходе трудно добиться значительного ускорения времени обучения, поскольку все вычисления выполняются последовательно. Необходимо также учитывать накладные расходы из-за значительной передачи данных между устройствами.

Параллелизм модели имеет смысл использовать в случае обучения огромных моделей, которые не помещаются на одном устройстве, даже при небольшом размере блоков.

![Model Parallelism](../.gitbook/assets/mp.png)

### Параллелизм данных

В отличии от параллелизма модели при данном подходе небольшие фрагменты данных распределяются между несколькими устройствами, т.е. пакет разбивается на маленькие фрагменты и обрабатывается независимо несколькими копиями одной и той же модели.

Организация обучения заключается в следующем. Одно из устройств предназначено для использования в качестве главного устройства, основной задачей которого является сбор градиентов для градиентного спуска с других устройств и обновление весовых коэффициентов для всех копий модели.

![Data Parallelism](../.gitbook/assets/dp.png)

Параллелизм данных является универсальным методом и предоставляется большинством популярных библиотек DL для обучения \(DistributedDataParallel в PyTorch\), освобождая пользователей от необходимости синхронизировать данные между устройствами.

## Попробуйте сами

В нашем [репозитории](https://github.com/neuromation/ml-recipe-distributed-pytorch), мы использовали подход параллелизма данных. Репозиторий содержит решение для [конкурса ответов на вопросы о TensorFlow 2.0](https://www.kaggle.com/c/tensorflow2-question-answering), проводимого Kaggle.

Чтобы загрузить набор данных [Google’s Natural Questions](https://ai.google.com/research/NaturalQuestions/dataset) и запустить наше решение, Вы должны принять пользовательское соглашение Kaggle. Если вы хотите повторно использовать наш рецепт в качестве шаблона для проведения распределенного обучения на Neu.ro, мы также предоставляем `DummyDataset`, который не требует загрузки каких-либо данных.

Для запуска рецепта из `DummyDataset`, необходимо выполнить следующие шаги:

1. [Зарегистрируйтесь](https://neu.ro/) и [установите клиент CLI](https://docs.neu.ro/getting-started#installing-cli)
2. Клонируйте [репозиторий](https://github.com/neuromation/ml-recipe-distributed-pytorch)
3. Выполните `make setup`
4. Выполните `scripts/run_distributed_on_platform.sh`

Данный подход является общим, поэтому Вы можете использовать наше решение в качестве шаблона для распределенного обучения Ваших моделей в PyTorch.

